from langchain_nvidia_ai_endpoints.embeddings import NeMoEmbeddings
# Connect to NeMo Retriever Embedding service (local deployment)
embedding_model = NeMoEmbeddings(
model="nvidia/llama-3.2-nv-embedqa-1b-v2",
# embedding model ID to use (1B param Llama-based QA embedder)
base_url="http://localhost:8080/v1", # base URL of the embedding NIM service (OpenAI-like API)
batch_size=16 # batch size for embedding
calls (tune based on your GPU)
)

embedding_model = NeMoEmbeddings(model="NV-Embed-QA-003",
nvidia_api_key="YOUR_API_KEY")

from langchain.vectorstores import FAISS
# Example document texts to index (in practice, load and chunk your actual data)
docs = ["NVIDIA H200 is the first GPU with 141 GB of HBM3e memory, delivering 4.8TB/s of bandwidth.","The NVIDIA Triton Inference Server supports multiple frameworks and provides an HTTP/GRPC endpoint for AI model serving.",
# ... (more documents)
]
# Create a FAISS vector store by embedding all documents via the NeMo embedding service
vector_store = FAISS.from_texts(docs, embedding_model)

from langchain_nvidia_ai_endpoints.reranking import NVIDIARerank
# Connect to NeMo Retriever Reranking service (local deployment)
reranker = NVIDIARerank(
model="nvidia/nv-rerankqa-mistral-4b-v3", # reranker model ID (3.5B param Mistral model fine-tuned for QA)
base_url="http://localhost:8000/v1"
# base URL for the reranking NIM (local service)
# If using hosted API: provide nvidia_api_key instead of base_url
)

query = "What interfaces does NVIDIA Triton support?"

# Step 1: Initial retrieval using vector similarity search
candidate_docs = vector_store.similarity_search(query, k=10)
print(f"Initial documents retrieved: {len(candidate_docs)}")

# Step 2: Neural reranking of the candidates
reranked_docs = reranker.compress_documents(documents=candidate_docs,
query=query)
print(f"Reranked top documents: {len(reranked_docs)}")

for doc in reranked_docs:
    score = doc.metadata.get("relevance_score", None)
    snippet = doc.page_content[:100] # first 100 characters of the doc
    print(f"Score: {score:.3f} | Passage: {snippet}...")

# This might output something like:

# Score: 16.625 | Passage: NVIDIA H200 Tensor Core GPU | Datasheet 1 – NVIDIA H200
# Tensor Core GPU supercharges...
# Score: 11.508 | Passage: NVIDIA H200 NVL is the ideal choice for customers with
# space constraints...
# Score: 8.258 | Passage: NVIDIA H200 Tensor Core GPU | Datasheet 2 – Memory
# bandwidth is crucial for HPC applications...



from langchain_nvidia_ai_endpoints import ChatNVIDIA
# Initialize an LLM (for example, a 70B Llama2 model hosted by NVIDIA API or a local NIM)
llm = ChatNVIDIA(model="ai-llama2-70b", max_tokens=500,
nvidia_api_key="YOUR_API_KEY")
# Construct a prompt with the top reranked context
context_text = "\n".join([doc.page_content for doc in reranked_docs])
prompt = f"Context:\n{context_text}\n\nQuestion: {query}\nAnswer:"
response = llm.invoke(prompt)
print(response.content)

